---
title: "Regression Analysis"
header-includes: \usepackage{booktabs} \usepackage{siunitx} \newcolumntype{d}{S[input-symbols = ()]}
output: pdf_document
---

```{r, include = F}
library(knitr)
opts_chunk$set(message = F, echo = F, out.width='\\textwidth')

library(plm)
library(pglm)
library(lme4)
library(MASS)
library(lme4)
library(vtable)
library(fixest)
library(lmtest)
library(GGally)
library(sandwich)
library(parameters)
library(modelsummary)

library(here)
source(here("functions.R"))

final_vars <-
    read_csv(here("clean-and-process", "src", "old_stuff", "final_data.csv")) %>%
    mutate(across(c("unit", "year", "unit"), ~as.factor(.x))) %>%
    mutate(log_total_officers = log(nr_officer),
           log_black_officers = log(nr_officer_black),
           white_stop_rate = round(white_stops / white * 10000),
           hispanic_stop_rate = round(hispanic_stops / black * 10000),
           log_black = log(black),
           property_cr_capita = property_cr * 10000 / total_pop,
           violent_cr_capita = violent_cr * 10000 /  total_pop,
           mean_years_exp = mean_days_exp / 365,
           mean_years_worked_unit = mean_months_worked_unit / 12)
```

## Corrrelation Matrix

```{r}
corr_vars <-
    final_vars %>%
    dplyr::select(black_stops, black_stop_rate, nr_officer, black, black_ratio,
                  mean_years_exp, violent_cr_capita, property_cr_capita) %>%
    rename(`Black Stops` = black_stops, `Black Stopping Rate` = black_stop_rate,
           `Nr. Officers` = nr_officer, `Black Pop.` = black,
           `Black racial congruence` = black_ratio,
           `Mean Years of Experience` = mean_years_exp,
           `Violent Crime Per 10,000` = violent_cr_capita,
           `Property Crime Per 10,000` = property_cr_capita)
 
ggcorr(corr_vars,
       low = "#F21A00",
       high = "#3B9AB2",
       label = T,
       label_round = 2,
       hjust = 0.75,
       size = 2)
```

## Summary Table

```{r, results = "asis"}
table_vars <-
    final_vars %>%
    dplyr::select(stops, black_stops, black_stop_rate, hispanic_stop_rate, white_stop_rate,
                  nr_officer, nr_officer_black,
                  prcnt_officer_black, total_pop, black, prcnt_civ_black,
                  black_ratio, mean_years_exp, violent_cr_capita, property_cr_capita)

labels_df <-
    tibble(vars = colnames(table_vars),
           labels = c("Total Number of Stops",
                      "Stops of Black Civilians",
                      "Stops of Black Civilians Per 10,000",
                      "Stops of White Civilians Per 10,000",
                      "Stops of Hispanic Civilians Per 10,000",
                      "Total Number of Officers",
                      "Total Number of Black Officers",
                      "Percent of Officers - Black",
                      "Total Population",
                      "Black Population",
                      "Percent of Population - Black",
                      "Black Racial Congruence",
                      "Mean Amount of Experience (Years)",
                      "Violent Crime Per 10,000 individuals",
                      "Property Crime Per 10,000 individuals"))

st(table_vars,
   summ = list(c('notNA(x)', 'mean(x)',   'sd(x)', 'min(x)', 'pctile(x)[25]', 
                 'median(x)', 'pctile(x)[75]', 'max(x)')),
   summ.names = list(c("N", "Mean", "Std. Dev.", "Min", "25th", "Median",
                     "75th", "Max")),
   labels = labels_df,
   title = "Summary Statistics Across All Units Over All Years")
```

```{r}
final_vars %>%
    filter(month == "6") %>%
    ggplot(aes(x = prcnt_civ_black, y = prcnt_officer_black)) +
    geom_point() +
    geom_abline() +
    facet_wrap(~year) +
    theme_bw() +
    labs(x = "Percentage of Population - Black",
         y = "Percentage of Police Unit - Black",
         title = "Percentage of the population that is Black vs. percentage of the police force that is Black") +
    theme(text = element_text(size = 25))
```

```{r}
race_exp <-
    final_vars %>%
    group_by(year, unit) %>%
    summarise(corr = cor(black_ratio, mean_years_exp)) %>%
    ungroup()

final_vars %>%
    filter(unit %in% c(1, 10, 6)) %>%
    ggplot(aes(x = black_ratio, y = mean_years_exp)) +
    geom_point(aes(color = year)) +
    facet_wrap(~unit, scales = "free") +
    theme_bw() +
    labs(x = "Black Racial Congruence", y = "Mean Years of Experience",
         title = "Racial Congruence vs. Mean Years of Experience for Units 1, 6, and 10")

final_vars %>%
    ggplot(aes(x = black_ratio, y = mean_years_exp)) +
    geom_point(aes(color = unit)) +
    theme_bw() +
    labs(x = "Black Racial Congruence", y = "Mean Years of Experience",
         title = "Black Racial Congruence vs. Mean Years of Experience") +
    geom_smooth(method = "lm",
                formula = y ~ x,
                se = F,
                aes(group = unit, color = unit)) +
    geom_smooth(method = "lm",
                formula = y ~ x,
                se = F,
                aes(group = 1))
```

```{r}
graph <-
    final_vars %>%
    dplyr::select(unit, year, month, white_stop_rate, black_stop_rate, hispanic_stop_rate,
                  black, white, hispanic) %>%
    group_by(unit) %>%
    summarise(white_stop_rate = median(white_stop_rate),
              black_stop_rate = median(black_stop_rate),
              hispanic_stop_rate = median(hispanic_stop_rate)) %>%
    ungroup() %>%
    pivot_longer(cols = matches("rate"), names_to = "Race", values_to = "Stops") %>%
    mutate(Race = case_when(Race == "white_stop_rate" ~ "White",
                            Race == "black_stop_rate" ~ "Black",
                            Race == "hispanic_stop_rate" ~ "Hisp.")) %>%
    group_by(unit) %>%
    ungroup() %>%
    group_by(unit) %>%
    mutate(max_match = Stops == max(Stops),
           max_match_black = if_else(max_match & Race == "Black", T, F),
           max_unit = any(max_match_black)) %>%
    ungroup()

sorting_var <-
    graph %>%
    filter(Race == "Black") %>%
    arrange(desc(max_match), desc(Stops))

graph <- graph %>% mutate(unit = factor(unit, levels = sorting_var$unit))

graph %>%
    ggplot(aes(x = Race, y = Stops)) +
    geom_line(aes(color = max_unit, group = unit)) +
    theme_bw() +
    labs(x = "Race", y = "Stopping Rate Per 10,000",
         title = "Median Stopping Rate By Race For Each Police Unit",
         color = "Stopping Rate Highest For Black Individuals?") +
    facet_wrap(~unit, scales = "free_y") +
    theme(text = element_text(size = 17))
```

## Unconditional Fixed Effects Negative Binomial Regression

* **Standard errors clustered on police unit**. We already allow for the error terms to be correlated across police units by including police unit as a dummy variable. I.e. We allow for observations to from the same unit to be correlated with themselves or more alike than we would expect if they were truly independent.
* However even within a police unit, observations are likely not truly independent of each other. Changes in unobserved factors are likely correlated over time. E.g. the police unit dummy variables represent time-invariant reasons why stops of Black civilians tend to be higher for some police units vs. others. However, **within a unit** there are likely time-varying unobserved reasons why the number of stops in one month is not truly independent of the previous month's or next month's number of stops.

### Why am I using an Unconditional Fixed Effects Negative Binomial Regression?
* Dependent variable is a count variable.
* An offset is included for the size of the Black population.
* Evidence of overdispersion. The conditional variance is not equal to the conditional mean.
* Unconditional nonlinear regression models suffer from the incidental parameter problem when the number of time-periods observed is small compared to the number of cross-sectional units.
* I have T = 36 time observation periods and N = 22 police unit observations. I don't think incidental parameters should be an issue.
* But just in case...
* "On the other hand, a simulation study yields good results from applying an unconditional negative binomial regression estimator with dummy variables to represent the fixed effects. There is no evidence for any incidental parameters bias in the coefficients..." Allison and Waterman 2002.

### Not enough variation in independent variable of interest over time
* If there isn't enough variation in the independent variable over time, this can hurt the power of the model or its ability to detect a true effect. It can become too sensitive to small changes in the data (since the data only encompasses small changes).
* It also may not be detecting the true effect of the variable as its effect may get absorbed by the unit effects.
* I show the effect of the variable is robust to different model specifications (i.e. including more variables). Descriptively, I think there is a relationship to be explored and there seems to be some variation in some units. There is no test which can specify before running the model if there is enough variation.

## Results
* Reported in log odds.
* Concerned with directionality not interpreting the coefficients.
* Use predictions from model to understand true effect size.

### Between vs. within

```{r, results = "asis"}
# https://www.andrewheiss.com/blog/2016/04/25/convert-logistic-regression-standard-errors-to-odds-ratios-with-r/
# I think the function I'm using also came from a stack overflow thread, same idea as above.
# made redundant by model summary

find_or_se <- function(model) {
    odds_ratio <- exp(coef(model))
    odds_ratio <- odds_ratio[!(names(odds_ratio) %in% c("(Intercept)"))]
    
    diag_vcov <- sqrt(diag(vcov(model)))
    diag_vcov <- diag_vcov[!(names(diag_vcov) %in% c("(Intercept)", ".theta"))]
    
    se <- odds_ratio * diag_vcov
    return(se)
}

final_vars  <-
    final_vars %>%
    group_by(unit) %>%
    mutate(between_black_ratio = mean(black_ratio),
           within_black_ratio = black_ratio - between_black_ratio) %>%
    ungroup()

between_within_count <-
    femlm(black_stops ~ between_black_ratio + within_black_ratio + year,
          family = "negbin",
          data = final_vars,
          cluster = "unit")
between_within_count_se <- find_or_se(between_within_count)

between_within_rate <-
    femlm(black_stops ~ between_black_ratio + within_black_ratio + year +
              offset(log(black)),
          family = "negbin",
          data = final_vars,
          cluster = "unit")
between_within_rate_se <- find_or_se(between_within_rate)

offset_row <- tibble(term = c("", "Offset - Black Pop."),
                     `Model 1` = c("Model 1", "No"),
                     `Model 2` = c("Model 2", "Yes"))
attr(offset_row, "position") <- c(1, 7)

modelsummary(list("Stops of Black Civilians" = between_within_count,
                  "Stops of Black Civilians" = between_within_rate),
             coef_omit = "(Intercept)|theta",
             coef_rename = c("between_black_ratio" = "Black Racial Congruence - Between Unit",
                            "within_black_ratio" = "Black Racial Congruence - Within Unit",
                            "black_stops" = "Stops of Black Civilians"),
             estimate = "{estimate} ({std.error}){stars}",
             statistic = NULL,
             stars = T,
             gof_omit = "R2$|R2 Adj|R2 Within",
             exponentiate = T,
             output = "latex",
             add_rows = offset_row,
             notes = c("Standard Errors in parentheses. Coefficients are incident rate ratios.",
                       "P-values are denoted by symbols: + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"))
```

```{r, results = "asis"}
fixest_cluster_nb_basic <-
    femlm(black_stops ~ black_ratio + violent_cr_capita + property_cr_capita + log_total_officers +
              year + offset(log(black)) | unit,
          family = "negbin",
          data = final_vars)

fixest_cluster_nb_exp <-
    femlm(black_stops ~ mean_years_exp + violent_cr_capita + property_cr_capita + log_total_officers +
              year + offset(log(black)) | unit,
          family = "negbin",
          data = final_vars)

fixest_cluster_nb_full <-
    femlm(black_stops ~ black_ratio + mean_years_exp + violent_cr_capita + property_cr_capita + log_total_officers +
              year + offset(log(black)) | unit,
          family = "negbin",
          data = final_vars)

offset_row <- tibble(term = c("", "Offset - Black Pop."),
                     `Model 1` = c("Model 1", "Yes"),
                     `Model 2` = c("Model 2", "Yes"))
attr(offset_row, "position") <- c(1, 9)

modelsummary(list("Stops of Black Civilians" = fixest_cluster_nb_basic,
                  "Stops of Black Civilians" = fixest_cluster_nb_exp,
                  "Stops of Black Civilians" = fixest_cluster_nb_full),
             coef_omit = "(Intercept)|theta",
             coef_rename = c(black_ratio = "Black Racial Congruence",
                             mean_years_exp = "Mean Amount of Experience (Years)",
                             violent_cr_capita = "Violent Crime Per 10,000",
                             property_cr_capita = "Property Crime Per 10,000",
                             log_total_officers = "Log of the Total Number of Officers"),
             estimate = "{estimate} ({std.error}){stars}",
             statistic = NULL,
             stars = T,
             gof_omit = "R2$|R2 Adj|R2 Within",
             exponentiate = T,
             output = "latex",
             add_rows = offset_row,
             notes = c("Standard Errors in parentheses. Coefficients are incident rate ratios.",
                       "P-values are denoted by symbols: + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"))
```

```{r}
offset_row <- tibble(term = c("", "Offset - Black Pop."),
                     `Model 1` = c("Model 7", "Yes"),
                     `Model 2` = c("Model 8", "Yes"),
                     `Model 3` = c("Model 9", "Yes"))
attr(offset_row, "position") <- c(1, 9)

modelsummary(list("Stops of Black Civilians" = fixest_cluster_nb_basic,
                  "Stops of Black Civilians" = fixest_cluster_nb_exp,
                  "Stops of Black Civilians" = fixest_cluster_nb_full),
             coef_omit = "(Intercept)|theta",
             coef_rename = c(black_ratio = "Black Racial Congruence",
                             mean_years_exp = "Mean Amount of Experience (Years)",
                             violent_cr_capita = "Violent Crime Per 10,000",
                             property_cr_capita = "Property Crime Per 10,000",
                             log_total_officers = "Log of the Total Number of Officers"),
             estimate = "{estimate} ({std.error}){stars}",
             statistic = NULL,
             stars = T,
             gof_omit = "R2$|R2 Adj|R2 Within",
             exponentiate = T,
             output = "latex",
             add_rows = offset_row,
             notes = c("Standard Errors in parentheses. Coefficients are incident rate ratios.",
                       "P-values are denoted by symbols: + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"))
```

### Plotting number of stops

* Scatterplot of Black Racial Congruence Vs. Number of Stops of Black Civilians.
* Overall trend suggests a negative relationship wherein increasing Black Racial Congruence is correlated with less stops of Black civilians.
* Differences in the the within unit trend though. Some units exhibit a negative relationship while other units exhibit no relationship. The variation in racial congruence also *varies* by unit making it hard to discern the trend (if any) in some of the units.
* The bottom graph makes it easier to see trends **within-unit**. There a few units with a positive relationship and there are a few others with no relationship, but most units demonstrate a slight to moderately negative relationship.
* As above table suggests though, there are a lot of moving parts. Black officers are being hired (or in some cases leaving), and the total size of the police force is changing **all** relative to the share of the population that is Black.
* Interpreting this graph, there is an association between the share of the police force that is Black and the share of population that is Black such that as this ratio increases (indicating parity and in a few cases over-representation), the number of stops of Black civilians declines.

```{r}
ggplot(final_vars, aes(y = black_stops, x = black_ratio)) +
    geom_point(aes(color = unit)) +
    geom_smooth(method = MASS::glm.nb,
                formula = y ~ x,
                se = F,
                aes(group = unit, color = unit)) +
    geom_smooth(method = MASS::glm.nb,
                formula = y ~ x,
                se = F,
                aes(group = 1)) +
    theme_bw() +
    labs(x = "Black Racial Congruence",
         y = "Number of Stops of Black Civilians") +
    theme(text = element_text(size = 19))
```

```{r}
ggplot(final_vars, aes(y = black_stops, x = black_ratio)) +
    geom_point(aes(color = unit)) +
    geom_smooth(method = MASS::glm.nb,
                formula = y ~ x,
                se = F,
                aes(group = unit, color = unit)) +
    facet_wrap(~unit, scales = "free") +
    theme_bw() +
    labs(x = "Black Racial Congruence",
         y = "Number of Stops of Black Civilians")
```

### Plotting Rate of Stops

```{r}
ggplot(final_vars, aes(y = black_stop_rate, x = black_ratio)) +
    geom_point(aes(color = unit)) +
    geom_smooth(method = MASS::glm.nb,
                formula = y ~ x,
                se = F,
                aes(group = unit, color = unit)) +
    geom_smooth(method = MASS::glm.nb,
                formula = y ~ x,
                se = F,
                aes(group = 1)) +
    theme_bw() +
    labs(x = "Black Racial Congruence",
         y = "Stops of Black Civilians Per 10,000") +
    theme(text = element_text(size = 19))

ggplot(final_vars, aes(y = black_stop_rate, x = black_ratio)) +
    geom_point(aes(color = unit)) +
    geom_smooth(method = MASS::glm.nb,
                formula = y ~ x,
                se = F,
                aes(group = unit, color = unit)) +
    facet_wrap(~unit, scales = "free") +
    theme_bw() +
    labs(x = "Black Racial Congruence",
         y = "Stops of Black Civilians Per 10,000")
```

## Residualized Regression

```{r}
residual_reg <- lm(mean_years_exp ~ black_ratio, data = final_vars)
residuals_exp <- residual_reg$residuals
final_vars <- final_vars %>% mutate(residual_exp = residuals_exp)
residual_reg_nb <-
    femlm(black_stops ~ black_ratio + residual_exp + violent_cr_capita +
              property_cr_capita + log_total_officers + year + offset(log(black)) | unit,
          family = "negbin",
          data = final_vars)

residual_reg2 <- lm(black_ratio ~ mean_years_exp, data = final_vars)
residuals_ratio <- residual_reg2$residuals
final_vars <- final_vars %>% mutate(residual_ratio = residuals_ratio)
residual_reg_nb2 <-
    femlm(black_stops ~ residual_ratio + mean_years_exp + violent_cr_capita +
              property_cr_capita + log_total_officers + year + offset(log(black)) | unit,
          family = "negbin",
          data = final_vars)
```


```{r}
offset_row <- tibble(term = c("", "Offset - Black Pop."),
                     `Model 1` = c("Model 3", "Yes"),
                     `Model 2` = c("Model 4", "Yes"))
attr(offset_row, "position") <- c(1, 11)

modelsummary(list("Stops of Black Civilians" = residual_reg_nb,
                  "Stops of Black Civilians" = residual_reg_nb2),
             coef_omit = "(Intercept)|theta",
             coef_rename = c(black_ratio = "Black Racial Congruence",
                             residual_exp = "Residualized Mean Amount of Experience",
                             residual_ratio = "Residualized Black Racial Congruence",
                             mean_years_exp = "Mean Amount of Experience (Years)",
                             violent_cr_capita = "Violent Crime Per 10,000",
                             property_cr_capita = "Property Crime Per 10,000",
                             log_total_officers = "Log of the Total Number of Officers"),
             estimate = "{estimate} ({std.error}){stars}",
             statistic = NULL,
             stars = T,
             gof_omit = "R2$|R2 Adj|R2 Within",
             exponentiate = T,
             output = "latex",
             add_rows = offset_row,
             notes = c("Standard Errors in parentheses. Coefficients are incident rate ratios.",
                       "P-values are denoted by symbols: + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"))
```

### Predictions To Aid In Interpretation of Coefficients

* Standardized coefficients do not make much sense to use in aiding in the interpretation of panel regressions. Should one standardize using the **total** mean and standard deviation? Within time unit mean and standard deviation? Within police unit? Within police unit crossed by time unit? Furthermore such interpretations in the context of a fixed effects regression remain unclear.
* To aid in the interpretation of the regression models, I systematically vary the value of the Black Racial Congruence variable (e.g. using maximum and minimum observed values for each unit within each year) while holding all other variables constant at their mean. I then do the same for the Mean Amount of Experience (Years) as a comparison point.

```{r}
nr_officers <-
    final_vars %>%
    dplyr::select(unit, year, black_ratio, nr_officer_black) %>%
    group_by(unit, year) %>%
    mutate(rank = row_number(black_ratio)) %>%
    mutate(type = case_when(rank == min(rank) ~ "min",
                            rank == max(rank) ~ "max",
                            rank == 4 ~ "p25",
                            rank == 6 ~ "median",
                            rank == 9 ~ "p75",
                            T ~ "drop")) %>%
    ungroup() %>%
    filter(type != "drop") %>%
     dplyr::select(-black_ratio, -rank)

new_data_ratio <-
    final_vars %>%
    group_by(unit, year, black) %>%
    summarise(prcnt_officer_FEMALE = mean(prcnt_officer_FEMALE),
              mean_years_exp = mean(mean_years_exp),
              violent_cr_capita = mean(violent_cr_capita),
              property_cr_capita = mean(property_cr_capita),
              log_total_officers = mean(log_total_officers),
              black = mean(black),
              min = min(black_ratio),
              max = max(black_ratio),
              median = median(black_ratio),
              p25 = quantile(black_ratio)[["25%"]],
              p75  = quantile(black_ratio)[["75%"]]) %>%
    pivot_longer(c("min", "max", "median", "p25", "p75"),
                 names_to = "type",
                 values_to = "black_ratio") %>%
    ungroup() %>%
    full_join(nr_officers, by = c("unit", "year", "type"))

new_data_exp <-
    final_vars %>%
    group_by(unit, year, black) %>%
    summarise(prcnt_officer_FEMALE = mean(prcnt_officer_FEMALE),
              black_ratio = mean(black_ratio),
              violent_cr_capita = mean(violent_cr_capita),
              property_cr_capita = mean(property_cr_capita),
              log_total_officers = mean(log_total_officers),
              black = mean(black),
              min = min(mean_years_exp),
              max = max(mean_years_exp),
              median = median(mean_years_exp),
              p25 = quantile(mean_years_exp)[["25%"]],
              p75 = quantile(mean_years_exp)[["75%"]]) %>%
    pivot_longer(c("min", "max", "median", "p25", "p75"),
                 names_to = "type",
                 values_to = "mean_years_exp") %>%
    ungroup()
```

### The effect of moving from the least amount of racial congruence to the most for each year for each unit

As mentioned above, I hold all other values constant at their means (for each unit in each year). I then systematically vary the value of Black racial congruence using the minimum value (for that unit in that year), the 25th percentile, the median, the 75th percentile, and the maximum value.

```{r}
predict_results_ratio_no_exp <-
    new_data_ratio %>%
    mutate(predicted_black_stops = predict(fixest_cluster_nb_basic,
                                           newdata = new_data_ratio) * black,
           type = factor(type, levels = c("min", "p25", "median", "p75", "max")),
           variable = "Black Racial Congruence (No Experience)")

predict_results_ratio <-
    new_data_ratio %>%
    mutate(predicted_black_stops = predict(fixest_cluster_nb_full,
                                           newdata = new_data_ratio) * black,
           type = factor(type, levels = c("min", "p25", "median", "p75", "max")),
           variable = "Black Racial Congruence")

predict_results_exp <-
    new_data_exp %>%
    mutate(predicted_black_stops = predict(fixest_cluster_nb_full,
                                           newdata = new_data_exp) * black,
           type = factor(type, levels = c("min", "p25", "median", "p75", "max")),
           variable = "Mean Years of Experience")

predict_results <-
    dplyr::select(predict_results_ratio, -nr_officer_black) %>%
    rbind(predict_results_exp)
```


```{r}
predict_results_both_models <-
    dplyr::select(predict_results_ratio, -nr_officer_black) %>%
    rbind(predict_results_exp) %>%
    rbind(dplyr::select(predict_results_ratio_no_exp, -nr_officer_black))

predict_results_diff <-
    predict_results_both_models %>%
    dplyr::select(unit, year, type, variable, predicted_black_stops) %>%
    filter(type == "min" | type == "max") %>%
    pivot_wider(id_cols = c("unit", "year", "variable"),
                names_from = type,
                values_from = c("predicted_black_stops")) %>%
    mutate(decrease_stops = min - max) %>%
    filter(year == 2014)

ggplot(predict_results_diff, aes(y = fct_reorder(unit, decrease_stops), x = decrease_stops)) + 
    geom_bar(aes(fill = variable), stat = "identity") +
    theme_bw() +
    labs(x = "Decrease in Stops (Moving From Minimum to Maximum Value)", y = "Unit",
         title = "Decrease in Predicted Number of Stops: Comparison of Effects")
```


```{r}
predict_results_ratio %>%
    ggplot(aes(x = type, y = predicted_black_stops)) +
    geom_point(aes(color = year)) +
    geom_line(aes(color = year, group = year)) +
    facet_wrap(~unit) +
    theme_bw() +
    labs(x = "Different Values of Racial Congruence", 
         y = "Predicted Number of Stops of Black Civilians")
```

In this first graph where the same scale is used for all observations, it's impossible to discern or see the changes because the changes are small and need to be contextualized within the unit in which they're happening.

```{r}
predict_results_ratio %>%
    ggplot(aes(x = type, y = predicted_black_stops)) +
    geom_point(aes(color = year)) +
    geom_line(aes(color = year, group = year)) +
    facet_wrap(~unit, scales = "free") +
    theme_bw() +
    labs(x = "Different Values of Racial Congruence", 
         y = "Predicted Number of Stops of Black Civilians")
```

In this second graph where the y-axis is allowed to freely change with the unit, we can discern changes. This allows us to see how different vales of racial congruence lead to a declining number of stops of Black civilians.

```{r}
predict_results_diff <-
    predict_results_ratio %>%
    dplyr::select(unit, year, type, black_ratio, nr_officer_black,
                  predicted_black_stops) %>%
    filter(type == "min" | type == "max") %>%
    pivot_wider(id_cols = c("unit", "year"),
                names_from = type,
                values_from = c("predicted_black_stops", "black_ratio",
                                "nr_officer_black")) %>%
    mutate(prcnt_decrease_stops = (predicted_black_stops_min - predicted_black_stops_max) / predicted_black_stops_min * 100,
           decrease_stops = predicted_black_stops_min - predicted_black_stops_max,
           prcnt_increase_ratio = (black_ratio_max - black_ratio_min) / black_ratio_min * 100,
           prcnt_chg_officer = (nr_officer_black_max - nr_officer_black_min) / nr_officer_black_min * 100,
           chg_officer = nr_officer_black_max - nr_officer_black_min)

predict_results_diff %>%
    filter(year == 2014) %>%
    ggplot(aes(x = fct_reorder(unit, prcnt_decrease_stops), y = prcnt_decrease_stops)) +
    geom_segment( aes(x = fct_reorder(unit, prcnt_decrease_stops),
                      xend = fct_reorder(unit, prcnt_decrease_stops),
                      y = 0, yend = prcnt_decrease_stops)) +
    geom_point( color="blue", size=4, alpha=0.6) +
    theme_bw() +
    labs(x = "Unit", y = "Percent Decrease in Predicted Black Stops")

predict_results_diff %>%
    filter(year == 2014) %>%
    ggplot(aes(x = fct_reorder(unit, decrease_stops), y = decrease_stops)) +
    geom_segment( aes(x = fct_reorder(unit, decrease_stops),
                      xend = fct_reorder(unit, decrease_stops),
                      y = 0, yend = decrease_stops)) +
    geom_point( color="blue", size=4, alpha=0.6) +
    theme_bw() +
    labs(x = "Unit", y = "Absolute Decrease in Predicted Black Stops")
```

The bar graphs demonstrate the percentage and absolute change in the number of stops within each unit for each year when the racial congruence measure starts at its minimum value but is then increased to its maximum value.

```{r}
predict_table <-
    predict_results_diff %>%
    filter(year == 2014) %>%
    dplyr::select(-prcnt_decrease_stops, -prcnt_increase_ratio,
                  -prcnt_chg_officer, -year) %>%
    mutate(across(matches("stops"), ~round(.x)),
           across(matches("ratio"), ~round(.x, 2))) %>%
    unite("predicted_stops",
          predicted_black_stops_min:predicted_black_stops_max,
          sep = " to ") %>%
    unite("black_ratio",
          black_ratio_min:black_ratio_max,
          sep = " to ") %>%
    unite("nr_officer_black",
          nr_officer_black_min:nr_officer_black_max,
          sep = " to ")
    
kable(predict_table,
      booktabs = T,
      col.names = c("Unit",
                    "Change - Pred. Black Stops",
                    "Min to Max, Racial Con.",
                    "Change - Black Officers",
                    "Diff - Stops",
                    "Diff - Officers"))  %>%
    kable_styling(full_width = F)
```

Here is a table trying to link together changes in racial congruence, the predicted number of stops of Black civilians, and changes in the absolute number of Black officers.

### Appendix results

```{r}
ggplot(predict_results_ratio, aes(x = black_ratio, y = predicted_black_stops)) +
    geom_point(aes(color = year)) + 
    geom_line(aes(color = year, group = year)) +
    facet_wrap(~unit, scales = "free") + 
    theme_bw() +
    labs(x = "Black Racial Congruence",
         y = "Predicted Number of Stops of Black Civilians")
```

Here instead of having the x-axis be categorical, I use the associated Black Racial Congruence measure. So each dot on a line moving from left to right (or lowest to highest Racial Congruence) represents the minimum, 25th percentile, median, 75th percentile, and maximum racial congruence for that unit in that year.

```{r}
ggplot(predict_results_ratio, aes(x = nr_officer_black, y = predicted_black_stops)) +
    geom_point(aes(color = year)) + 
    geom_line(aes(color = year, group = year)) +
    facet_wrap(~unit, scales = "free") + 
    theme_bw() +
    labs(x = "Number of Black Officers",
         y = "Predicted Number of Stops of Black Civilians")
```

This graph is a bit more complicated. The x-axis is the **rough** association of the number of Black officers to the corresponding values of racial congruence. I say rough because there isn't always an **exact** match to the racial congruence measures because the 25th percentile is averaged between two values, the median is averaged between two values, and the 75th percentile is averaged between two values.

In general, an increasing number of Black officers is associated with less stops of Black civilians. Occasionally though, an increasing number of Black officers is associated with an increasing number of stops of Black civilians. This has to do with the fact that even though the number of Black officers increased, the racial congruence decreased. I.e. the total number of officers increased at a higher rate than the rate of increase for Black officers. Their share or percentage of the force was diluted.

```{r}
predict_results %>%
    filter(year == 2014) %>%
    ggplot(aes(x = type, y = predicted_black_stops)) +
    geom_point(aes(color = variable)) +
    geom_line(aes(color = variable, group = variable)) +
    facet_wrap(~unit, scales = "free_y") +
    theme_bw() +
    labs(x = "Percentiles", y = "Predicted Number of Stops of Black Civilians") +
    theme(text = element_text(size = 14))
```

Demonstrates how racial congruence compares to the mean amount of experience in a police unit within a given year and how strongly associated each variable is with the decrease in the number of stops of Black civilians. Experience has the stronger association.

```{r}
library(PASSED)

predict_results_diff_exp <-
    predict_results_exp %>%
    dplyr::select(unit, year, type, mean_years_exp, predicted_black_stops) %>%
    filter(type == "min" | type == "max") %>%
    pivot_wider(id_cols = c("unit", "year"),
                names_from = type,
                values_from = c("predicted_black_stops")) %>%
    mutate(prcnt_decrease_stops = (min - max) / min * 100,
           decrease_stops = min - max)

power1 <-
    power_NegativeBinomial(n1 = nrow(final_vars),
                           mu2 = mean(final_vars$black_stops),
                           mu1 = mean(final_vars$black_stops) - mean(predict_results_diff$decrease_stops),
                           theta = summary(fixest_cluster_nb_full)$theta)

power2 <-
    power_NegativeBinomial(n1 = nrow(final_vars),
                           mu1 = mean(final_vars$black_stops),
                           mu2 = mean(final_vars$black_stops) - mean(predict_results_diff_exp$decrease_stops),
                           theta = summary(fixest_cluster_nb_full)$theta)
```
