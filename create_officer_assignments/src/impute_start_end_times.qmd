---
title: "Impute missing start and end times for work assignments"
format: pdf
---

```{r, include = F}
library(here)
library(knitr)
library(readr)
library(dplyr)
library(purrr)
library(dtplyr)
library(ggplot2)
library(stringr)
library(flextable)
opts_chunk$set(message = F, echo = F, warning = F)
```

## The logic and assumptions behind imputation

The key assumption being made is that officer-work assignments with a missing start time and end time will have the same start and end times as other **similar** officer-work assignments. If there was a systematic reason for the officer-work assignments to be missing their start and end times though, this assumption is not valid, and the imputation will not be valid.

Some of the missing officer-work assignments will be unique (i.e. there are not other similar officer work-assignments) in which case no values can be imputed for that officer-work assignment.

**Similarity** of officer-work assignments is systematically varied (e.g shift timing and beat **or** shift timing, beat, and month **or** shift timing, beat, month, and day of week **or** officer, beat, and shift timing), and the most common start and end times for officer-work assignments within each of these groupings is found.

As an example, officer 16811 was assigned to beat 110 on October 1, 2012 (Monday) during the 2nd shift. However, this officer-work assignment is missing its start and end times. So we look at all other officer-work assignments which took place in this beat during this shift timing and find the most common start/end times for those officer-work assignments. The most common start/end time (5:30am - 2:30pm) would then be the value given to this officer-work assignment missing its start/end time. Of course, one could argue one should only look at officer-work assignments which took place in this beat during this shift timing in the *month of October*. One might also argue you should only look at this specific officer and observe their start/end times when they work this beat during this specific shift timing. One can imagine many different ways to conceptualize **similar** sets of officer-work assignments to compare to.

What I will attempt to do here is try and disambiguate between all the different imputation possibilities.

```{r}
# Read in merged officer-work assignments.
assignments <-
    read_csv(
        here("create_officer_assignments", "input", "assignments.csv.gz"),
        col_types =
            cols_only(
                officer_id = "c",
                month = col_date(format = "%Y-%m-%d"),
                rank = "c",
                unit = "c",
                date = col_date(format = "%Y-%m-%d"),
                shift = "c",
                start_time = "d",
                end_time = "d",
                weekday = "c",
                beat_assigned = "c",
                months_from_start = "d",
                months_from_start_sq = "d",
                duration = "d"
            )
    ) %>%
    mutate(
        end_t =
            case_when(
                is.na(end_time) ~ NA_real_,
                end_time > 24 ~ end_time - 24,
                TRUE ~ end_time
            ),
        row_id = as.character(row_number())
    )
```

```{r}
# Find most common start/end times for the grouping variables proposed
find_start_end_times <- function(df, grouping_vars, group_name) {
    df %>%
        lazy_dt() %>%
        filter(!is.na(start_time) & !is.na(end_time)) %>%
        count(
            pick(all_of(c(grouping_vars, 'start_time', 'end_time', "end_t"))),
            name = "nr_groupings_start_end"
        ) %>%
        group_by(pick(all_of(grouping_vars))) %>%
        mutate(
            prcnt = nr_groupings_start_end / sum(nr_groupings_start_end),
            nr_groupings = sum(nr_groupings_start_end)
        ) %>%
        filter(prcnt == max(prcnt)) %>%
        ungroup() %>%
        distinct(pick(all_of(grouping_vars)), .keep_all = T) %>%
        mutate(group = group_name) %>%
        as_tibble()
}

groupings <-
    list(
        sb = c('shift', 'beat_assigned'),
        sbm = c('shift', 'beat_assigned', 'month'),
        sbmw = c('shift', 'beat_assigned', 'month', 'weekday'),
        sbd = c("shift", "beat_assigned", "date"),
        sbu = c('shift', 'beat_assigned', 'unit'),
        sbmu = c('shift', 'beat_assigned', 'month', 'unit'),
        sbmwu = c('shift', 'beat_assigned', 'month', 'weekday', 'unit'),
        sbdu = c("shift", "beat_assigned", "date", 'unit'),
        obs = c("officer_id", "beat_assigned", "shift"),
        obsm = c("officer_id", "beat_assigned", "shift", "month"),
        obsmw = c('officer_id', "beat_assigned", "shift", "month", "weekday"),
        obsu = c("officer_id", "beat_assigned", "shift", "unit"),
        obsmu = c("officer_id", "beat_assigned", "shift", "month", "unit"),
        obsmwu = c('officer_id', "beat_assigned", "shift", "month", "weekday", "unit")
    )

imputed <-
    pmap(
        list(groupings, as.list(names(groupings))),
        find_start_end_times,
        df = assignments
    )
```

## Evaluating different similiarity sets

Graphed below are `r length(groupings)` different ways I thought of to group together officer-work assignments. For each grouping, I visualize how well the imputation process works, in some sense. Using Shift + Beat as as an example, I will explain how to interpret the graph. First, I put every officer-work assignment into a group based on the beat and the shift timing. I.e., any officer-work assignment assigned to the same beat during the same shift timing is a part of the same group. Within each of these groups (based on the shift timing + beat), I found the most common start/end time for all officer-work assignments. Next, by assuming all officer-work assignments in a specific grouping had the most common start/end time, I calculated what percentage of the time the assumption is correct (x-axis). Then, I looked at how many officer-work assignments had the most common start/end time (y-axis). Finally, I combine groups if they have the exact same percentage and number of officer-work assignments with the most common start/end time (size of the circles). This last step is mostly to keep the graphic somewhat clean and not have so many overlapping dots.

As an example, look at shift + beat. Within shift + beat groups on average, 85% of officer-work assignments (or 319 officer-work assignments) have the most common start/end time. A specific dot represents a set of shift + beat groups where Y number of officer-work assignments (or X% of all officer-work assignments in those groups) share the most common start/end time. The size of the circle represents the total number of officer-work assignments in the set of groups (regardless of if they share the most common start/end time).

The main takeaway from the graph is the fact that as one increases the exclusivity of the groups, the higher the percentage of officer-work assignments which share the most common start/end time (meaning if we were to impute a missing value, we would be highly confident the start/end time would be correct). However, this mechanically means there is a smaller absolute number of officer-work assignments which we are comparing against, and in the worst case there may only be a group of 1 in which case it would not be helpful for imputation at all.

Also, in an absolute sense, we have can have some confidence that even in the worst case, we are still about 85% likely to be correct in our imputation (see Shift + Beat).

```{r, fig.width = 14, fig.height = 16}
# Evaluate how well each grouping variable performs.
avg_performance <-
    pmap(
        list(imputed, as.list(names(imputed))),
        function(df, group_name) {
            df %>%
                summarise(
                    mean_prcnt = weighted.mean(prcnt, nr_groupings),
                    mean_nr_groupings = mean(nr_groupings),
                    mean_nr_groupings_start_end = mean(nr_groupings_start_end),
                    group = group_name
                )
        }
    ) %>%
    bind_rows()

evaluate_imputation <-
    imputed %>%
    bind_rows() %>%
    count(prcnt, nr_groupings_start_end, group, wt = nr_groupings, name = "size") %>%
    full_join(avg_performance, by = "group") %>%
    mutate(
        pretty_group =
            case_when(
                group == "sb" ~ "Shift + Beat", 
                group == "sbm" ~ "Shift + Beat + Month",
                group == "sbmw" ~ "Shift + Beat + Month + Day of the week",
                group == "sbd" ~ "Shift + Beat + Date",
                group == "sbu" ~ "Shift + Beat + Unit",
                group == "sbmu" ~ "Shift + Beat + Month + Unit",
                group == "sbmwu" ~ "Shift + Beat + Month + Day of the week + Unit",
                group == "sbdu" ~ "Shift + Beat + Date + Unit",
                group == "obs" ~ "Officer + Beat + Shift",
                group == "obsm" ~ "Officer + Beat + Shift + Month", 
                group == "obsmw" ~ "Officer + Beat + Shift + Month + Day of the week",
                group == "obsu" ~ "Officer + Beat + Shift + Unit",
                group == "obsmu" ~ "Officer + Beat + Shift + Month + Unit",
                group == "obsmwu" ~ "Officer + Beat + Shift + Month + Day of the week + Unit"
            ),
        pretty_group =
            paste0(
                pretty_group, "\n", "Mean % w/ most common start/end: ", round(mean_prcnt, 3), "\n",
                "Mean # w/ most common start/end: ", round(mean_nr_groupings_start_end, 3)
            )
    ) %>%
    arrange(desc(mean_prcnt)) %>%
    mutate(
        pretty_group = factor(pretty_group, levels = unique(pretty_group))
    )

ggplot(evaluate_imputation, aes(x = prcnt, y = nr_groupings_start_end)) +
    geom_point(aes(size = size), alpha = 0.1) +
    theme_bw() +
    facet_wrap(~pretty_group, scales = "free_y", ncol = 2) +
    labs(
        x = "% of officer-work assignments w/ most common start/end time per group",
        y = "# of officer-work assignments w/ most common start/end time per group",
        size = "# of officer-work assignments in total"
    ) +
    theme(
        legend.title = element_text(size = 13),
        axis.title = element_text(size = 13),
        strip.text = element_text(size = 11),
        legend.text = element_text(size = 13),
        axis.text = element_text(size = 13)
    )
```

Using the above process of imputation, how far off the mark are we when we impute the wrong start/end time? The amount we are off the mark does not vary greatly with how we define similarity sets. On average, the imputation is off by about 1 hour to 1.5 hours with 75% of values between 0 and 2 hours. It is hard to say in an absolute sense how good/bad this is. It does not seem that bad?

```{r}
# When the predicted value does not match, how far off the mark is it?
deviation <-
    pmap(
        list(imputed, groupings, as.list(names(imputed))),
        function(imputed, group_vars, group_name, df) {
            df %>%
                filter(!is.na(start_time) | !is.na(end_time)) %>%
                left_join(imputed, by = group_vars) %>%
                mutate(
                    diff_start = abs(start_time.x - start_time.y),
                    diff_start = if_else(diff_start > 12, abs(diff_start - 24), diff_start),
                    diff_end = abs(end_t.x - end_t.y),
                    diff_end = if_else(diff_end > 12, abs(diff_end - 24), diff_end)
                ) %>%
                filter(!near(diff_start, 0) & !near(diff_end, 0)) %>%
                mutate(group = group_name)
        },
        df = assignments
    ) %>%
    bind_rows()

avg_deviation <-
    deviation %>%
    group_by(group) %>%
    summarise(
        min_start = min(diff_start),
        q1_start = quantile(diff_start)[["25%"]],
        median_start = median(diff_start),
        mean_start = mean(diff_start),
        q3_start = quantile(diff_start)[["75%"]],
        max_start = max(diff_start),
        min_end = min(diff_end),
        q1_end = quantile(diff_end)[["25%"]],
        median_end = median(diff_end),
        mean_end = mean(diff_end),
        q3_end = quantile(diff_end)[["75%"]],
        max_end = max(diff_end),
    ) %>%
    ungroup()

ggplot(deviation, aes(x = diff_start)) +
    geom_histogram() +
    facet_wrap(~group, scales = "free_y") +
    theme_bw()

ggplot(deviation, aes(x = diff_end)) +
    geom_histogram() +
    facet_wrap(~group, scales = "free_y") +
    theme_bw()

avg_deviation %>%
    mutate(across(is.numeric, ~ round(.x, 2))) %>%
    flextable() %>%
    padding(padding = 0, part = "all") %>%
    autofit() %>%
    fit_to_width(6.5)
```

## Which method to use?

```{r}
missings <-
    pmap(
        list(imputed, groupings, as.list(names(imputed))),
        function(imputed, group_vars, name, df) {
            df %>%
                filter(is.na(start_time) & is.na(end_time)) %>%
                left_join(imputed, by = group_vars) %>%
                select(
                    nr_groupings, nr_groupings_start_end, prcnt, start_time.y,
                    end_time.y, row_id
                ) %>%
                mutate(group = name)
             },
        df = assignments
    ) %>%
    bind_rows()

missings_match <-
    missings %>%
    rename(start_time = start_time.y, end_time = end_time.y) %>%
    filter(!is.na(start_time) & !is.na(end_time)) %>%
    group_by(row_id) %>%
    mutate(match = length(unique(start_time)) == 1 & length(unique(end_time)) == 1) %>%
    ungroup()

cannot_match <-
    missings %>%
    filter(is.na(start_time.y) & is.na(end_time.y)) %>%
    count(row_id) %>%
    filter(n == length(groupings))
```

As a refresher, we have `r table(is.na(assignments$start_time))[["TRUE"]]` officer-work assignments with a missing start/end time (out of `r nrow(assignments)` officer-work assignments). For each of these missing officer-work assignments, we impute a start/end time using the most common start/end time based on the group that the officer-work assignment is a part of (where group is defined in `r length(groupings)` different ways). How often do these estimates align? Quite a lot, thankfully. Roughly, `r round(prop.table(table(distinct(missings_match, row_id, .keep_all = T)$match))[["TRUE"]], 3) * 100`% of all missing officer-work assignments (which **can** be imputed) have the same estimated start/end time across all `r length(groupings)` methods.

The question now is should we try and impute the rest? If we do, what method should we use? One idea I had was:

* Select those estimates which come from those grouping definitions that have the highest percentage of officer-work assignments sharing in the most common start/end time. **NOTE**: Undecided on if we should include some sort of cutoff like the percentage has to be greater than 85%.
* If all estimates still do not match, keep those estimates from those grouping definitions with the highest absolute number of officer-work assignments that share in the most common start/end time.
* If all estimates still do no match, assert that some groupings are preferred over other groupings, all else equal, and use the estimates from that grouping.

We could also take the minimum start time of the estimates and the maximum end time of the estimates to create our estimate. We also may decide not to impute these officer-work assignments.

As a final note, there are `r nrow(cannot_match)` officer-work assignments which cannot be imputed. They have no other similar officer-work assignments regardless of the grouping definition.

## Do shift assignments with missing start/end times look like non-missing shift assignments?

* Race largely looks the same. It would appear as if slightly more Black officers have missing officer-work assignments.
* Gender looks the same.
* Age looks the same.
* Spanish-speakers look the same.
* Disproportionately, units 3 and 8 have the most officer-work assignments with missing times.
* Weekday distributions don't completely align, but I am not concerned.
* Missing shift times occur almost entirely in 2012 which is an intriguing finding.
* Shift timing distribution looks roughly the same with the 1st shift being slightly underrepresented.
* Rank distribution looks largely the same.

```{r}
officers <- 
    read_csv(
        here("create_officer_assignments", "input", "officers.csv.gz"),
        col_types = 
            cols_only(
                birth_year = "d",
                appointed_month = col_date(format = "%Y-%m-%d"),
                officer_id = "c",
                officer_race = "c",
                officer_gender = "c",
                spanish = "l"
            )
        )

officer_assignments_graph <-
    assignments %>%
    inner_join(officers, by = "officer_id") %>%
    mutate(missing = if_else(is.na(start_time), "Missing", "Not Missing"))

officer_assignments_graph %>%
    filter(!is.na(officer_race)) %>%
    mutate(officer_race = str_replace(officer_race, "officer_", "")) %>%
    count(officer_race, missing) %>%
    group_by(missing) %>%
    mutate(prcnt = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x = officer_race, y = prcnt)) +
    geom_bar(stat = "identity") +
    facet_wrap(~missing) +
    theme_bw()

officer_assignments_graph %>%
    filter(!is.na(officer_gender)) %>%
    count(officer_gender, missing) %>%
    group_by(missing) %>%
    mutate(prcnt = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x = officer_gender, y = prcnt)) +
    geom_bar(stat = "identity") +
    facet_wrap(~missing) +
    theme_bw()

officer_assignments_graph %>%
    filter(!is.na(birth_year)) %>%
    ggplot(aes(x = birth_year)) +
    geom_density() +
    facet_wrap(~missing, scales = "free_y") +
    theme_bw()

officer_assignments_graph %>%
    count(unit, missing) %>%
    group_by(missing) %>%
    mutate(prcnt = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x = unit, y = prcnt)) +
    geom_bar(stat = "identity") +
    facet_wrap(~missing) +
    theme_bw() +
    coord_flip()

officer_assignments_graph %>%
    count(weekday, missing) %>%
    group_by(missing) %>%
    mutate(prcnt = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x = weekday, y = prcnt)) +
    geom_bar(stat = "identity") +
    facet_wrap(~missing) +
    theme_bw()

officer_assignments_graph %>%
    filter(!is.na(date)) %>%
    ggplot(aes(x = date)) +
    geom_density() +
    facet_wrap(~missing, scales = "free_y") +
    theme_bw()

officer_assignments_graph %>%
    count(shift, missing) %>%
    group_by(missing) %>%
    mutate(prcnt = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x = shift, y = prcnt)) +
    geom_bar(stat = "identity") +
    facet_wrap(~missing) +
    theme_bw()

officer_assignments_graph %>%
    count(spanish, missing) %>%
    group_by(missing) %>%
    mutate(prcnt = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x = spanish, y = prcnt)) +
    geom_bar(stat = "identity") +
    facet_wrap(~missing) +
    theme_bw()

officer_assignments_graph %>%
    filter(!is.na(rank)) %>%
    mutate(
        rank =
            case_when(
                is.na(rank) ~ NA_character_,
                rank %in% c("CHIEF", "COMMANDER", "DEPUTY CHIEF", "LIEUTENANT") ~ "leadership",
                !(rank %in% c("CHIEF", "COMMANDER", "DEPUTY CHIEF", "LIEUTENANT", "POLICE OFFICER", "SERGEANT")) ~ "other",
                T ~ rank
            )
        ) %>%
    count(rank, missing) %>%
    group_by(missing) %>%
    mutate(prcnt = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x = rank, y = prcnt)) +
    geom_bar(stat = "identity") +
    facet_wrap(~missing) +
    theme_bw() +
    coord_flip()
```
